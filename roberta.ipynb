{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT, roBERTa.ipynb","provenance":[],"collapsed_sections":["JnlEP5U0DQmN","FPMlfDIhD7mK","tj5E7uVAEXoZ"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Hugging Face를 활용한 Modeling\n","Google Colab과 Pytorch, Hugging Face Trainer를 사용하여 Model Fine-tuning과 Inference를 진행해보자"],"metadata":{"id":"suHpaAp_X52A"}},{"cell_type":"markdown","source":["## EDA(Exploratory Data Analysis)\n","간단하게 Train, Test Dataset의 구조와 문장 분포를 확인해보았습니다."],"metadata":{"id":"PsWVN24mCsUR"}},{"cell_type":"markdown","source":["### Import Package\n","데이터 확인을 위한 패키지 불러오기"],"metadata":{"id":"jpY-hSPzYDpj"}},{"cell_type":"code","source":["import os\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"JeyMWkWvX4q2","executionInfo":{"status":"ok","timestamp":1645172360712,"user_tz":-360,"elapsed":5,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Load Data\n","Data가 위치한 PATH에서 Data를 불러오기"],"metadata":{"id":"vvKsvMWUDE85"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfzz9ALAv5Xo","executionInfo":{"status":"ok","timestamp":1645172385315,"user_tz":-360,"elapsed":24607,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"8eb0e0ad-45e9-44a8-8479-62e384075e7c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ldTRvy0bXqCz","outputId":"5574adfc-c8a1-408d-d550-c2ff4fd6b71f","executionInfo":{"status":"ok","timestamp":1645172386989,"user_tz":-360,"elapsed":1676,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-208bb92f-ae04-4b28-a3df-8c3bb347144a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>premise</th>\n","      <th>hypothesis</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n","      <td>씨름의 여자들의 놀이이다.</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n","      <td>자작극을 벌인 이는 3명이다.</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n","      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n","      <td>원주민들은 종합대책에 만족했다.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n","      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-208bb92f-ae04-4b28-a3df-8c3bb347144a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-208bb92f-ae04-4b28-a3df-8c3bb347144a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-208bb92f-ae04-4b28-a3df-8c3bb347144a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   index  ...          label\n","0      0  ...  contradiction\n","1      1  ...  contradiction\n","2      2  ...     entailment\n","3      3  ...        neutral\n","4      4  ...        neutral\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":3}],"source":["PATH =  '/content/drive/Shareddrives/Dacon/'\n","\n","train = pd.read_csv(os.path.join(PATH,'data/benchmark_train_data.csv'), encoding='utf-8')\n","test = pd.read_csv(os.path.join(PATH, 'data/test_data.csv'), encoding='utf-8')\n","\n","train.head(5)"]},{"cell_type":"markdown","source":["### Train, Test Data 확인"],"metadata":{"id":"JnlEP5U0DQmN"}},{"cell_type":"code","source":["print(train.info(), end='\\n\\n')\n","print(test.info())"],"metadata":{"id":"JaS09wEdrBqK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645172386989,"user_tz":-360,"elapsed":5,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"7f7d34d3-431d-4118-fd76-1695ec1862cb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 27998 entries, 0 to 27997\n","Data columns (total 4 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   index       27998 non-null  int64 \n"," 1   premise     27998 non-null  object\n"," 2   hypothesis  27998 non-null  object\n"," 3   label       27998 non-null  object\n","dtypes: int64(1), object(3)\n","memory usage: 875.1+ KB\n","None\n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1666 entries, 0 to 1665\n","Data columns (total 4 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   index       1666 non-null   int64 \n"," 1   premise     1666 non-null   object\n"," 2   hypothesis  1666 non-null   object\n"," 3   label       1666 non-null   object\n","dtypes: int64(1), object(3)\n","memory usage: 52.2+ KB\n","None\n"]}]},{"cell_type":"code","source":["print('Train Columns: ', train.columns)\n","print('Test Columns: ', test.columns)"],"metadata":{"id":"DvSnwwf-DWyT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645172386989,"user_tz":-360,"elapsed":4,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"69d50638-5226-4eb8-c180-ba000b06fde1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n","Test Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n"]}]},{"cell_type":"code","source":["print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')\n","print('Test Label: ', test['label'].value_counts(), sep='\\n')"],"metadata":{"id":"jdVnI8KTDZLJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645172386989,"user_tz":-360,"elapsed":3,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"226a234f-c93a-42d1-8ca8-9ed269b9d02b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Label: \n","entailment       9561\n","contradiction    9489\n","neutral          8948\n","Name: label, dtype: int64\n","\n","Test Label: \n","answer    1666\n","Name: label, dtype: int64\n"]}]},{"cell_type":"code","source":["print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n","print('Test Null: ', test.isnull().sum(), sep='\\n')"],"metadata":{"id":"vzHatuecpFmz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645172386990,"user_tz":-360,"elapsed":4,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"3ef00a2b-7c99-44ee-b176-fbd85f823540"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Null: \n","index         0\n","premise       0\n","hypothesis    0\n","label         0\n","dtype: int64\n","\n","Test Null: \n","index         0\n","premise       0\n","hypothesis    0\n","label         0\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["### Label 분포\n","Train Dataset의 Label 분포를 Bar Chart를 사용하여 시각화"],"metadata":{"id":"wJqxpG8LDyep"}},{"cell_type":"code","source":["feature = train['label']\n","\n","plt.figure(figsize=(10,7.5))\n","plt.title('Label Count', fontsize=20)\n","\n","temp = feature.value_counts()\n","plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n","plt.text(-0.05, temp.values[0]+20, s=temp.values[0])\n","plt.text(0.95, temp.values[1]+20, s=temp.values[1])\n","plt.text(1.95, temp.values[2]+20, s=temp.values[2])\n","\n","plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n","plt.show() # 그래프 나타내기"],"metadata":{"id":"-fe3NNsxqO7t","colab":{"base_uri":"https://localhost:8080/","height":506},"executionInfo":{"status":"ok","timestamp":1645172386990,"user_tz":-360,"elapsed":4,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"5f06676a-6f21-49a9-a8c3-124a37179c6a"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x540 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### 전제, 가설 길이 확인\n","Train에 존재하는 Premise(전제), Hypothesis(가설)의 길이를 확인하고 이를 통해 Tokenizer의 max_length 설정이 가능"],"metadata":{"id":"FPMlfDIhD7mK"}},{"cell_type":"code","source":["max_len = np.max(train['premise'].str.len())\n","min_len = np.min(train['premise'].str.len())\n","mean_len = np.mean(train['premise'].str.len())\n","\n","print('Max Premise Length: ', max_len)\n","print('Min Premise Length: ', min_len)\n","print('Mean Premise Lenght: ', mean_len, '\\n')\n","\n","max_len = np.max(train['hypothesis'].str.len())\n","min_len = np.min(train['hypothesis'].str.len())\n","mean_len = np.mean(train['hypothesis'].str.len())\n","\n","print('Max Hypothesis Length: ', max_len)\n","print('Min Hypothesis Length: ', min_len)\n","print('Mean Hypothesis Lenght: ', mean_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bV2WNMZQpPIy","outputId":"173ae9c1-c660-480d-a391-b02aa6f03b51","executionInfo":{"status":"ok","timestamp":1645172387557,"user_tz":-360,"elapsed":570,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Max Premise Length:  90\n","Min Premise Length:  19\n","Mean Premise Lenght:  45.32966640474319 \n","\n","Max Hypothesis Length:  103\n","Min Hypothesis Length:  5\n","Mean Hypothesis Lenght:  24.96296164011715\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","plt.figure(figsize=(10,7.5))\n","plt.title('Premise Length', fontsize=20)\n","\n","plt.hist(train['premise'].str.len(), alpha=0.5, color='orange')\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n","\n","plt.show()"],"metadata":{"id":"1nQ6JvawJ1B4","colab":{"base_uri":"https://localhost:8080/","height":506},"executionInfo":{"status":"ok","timestamp":1645172387557,"user_tz":-360,"elapsed":5,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"a67a33f3-146e-487a-958a-66ae8080ba56"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x540 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### 간단한 Test Preprocessing\n","전제, 가설에 존재하는 한글 단어가 아닌 다른 단어들은 전부 제거해줍니다."],"metadata":{"id":"tj5E7uVAEXoZ"}},{"cell_type":"code","source":["train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n","test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n","train.head(5)"],"metadata":{"id":"aBbzHrrfLYuX","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1645172387558,"user_tz":-360,"elapsed":5,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"a30f964d-04f7-482e-98bb-74388a1d71f0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-397ec362-648c-4c16-bc16-964711adf089\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>premise</th>\n","      <th>hypothesis</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n","      <td>씨름의 여자들의 놀이이다.</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n","      <td>자작극을 벌인 이는 3명이다.</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n","      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n","      <td>원주민들은 종합대책에 만족했다.</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n","      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-397ec362-648c-4c16-bc16-964711adf089')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-397ec362-648c-4c16-bc16-964711adf089 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-397ec362-648c-4c16-bc16-964711adf089');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   index  ...          label\n","0      0  ...  contradiction\n","1      1  ...  contradiction\n","2      2  ...     entailment\n","3      3  ...        neutral\n","4      4  ...        neutral\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n","test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n","train.head(5)"],"metadata":{"id":"MCFpm8BwMfao","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1645172387558,"user_tz":-360,"elapsed":5,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"15a82b6f-f4ce-4c67-a732-319d10467de9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-0306ac96-3181-4f66-bb70-75456e07238a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>premise</th>\n","      <th>hypothesis</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n","      <td>씨름의 여자들의 놀이이다</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n","      <td>자작극을 벌인 이는 3명이다</td>\n","      <td>contradiction</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n","      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다</td>\n","      <td>entailment</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n","      <td>원주민들은 종합대책에 만족했다</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n","      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0306ac96-3181-4f66-bb70-75456e07238a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0306ac96-3181-4f66-bb70-75456e07238a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0306ac96-3181-4f66-bb70-75456e07238a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   index  ...          label\n","0      0  ...  contradiction\n","1      1  ...  contradiction\n","2      2  ...     entailment\n","3      3  ...        neutral\n","4      4  ...        neutral\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Modeling"],"metadata":{"id":"sTDAsjbeEjIJ"}},{"cell_type":"markdown","source":["### Download transformers and Import Package"],"metadata":{"id":"Ws3oFvBkEpIV"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"3gCodaAYMnHO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645172395823,"user_tz":-360,"elapsed":8269,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"d1ab3916-32c0-478b-864c-a70ccee13707"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 69.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 51.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from transformers import TrainingArguments, Trainer\n","from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n","from transformers import TFBertForMaskedLM"],"metadata":{"id":"6Xhz9OFjEvzm","executionInfo":{"status":"ok","timestamp":1645172406844,"user_tz":-360,"elapsed":11023,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Seed 고정, GPU 설정"],"metadata":{"id":"Jh16K_pjFS11"}},{"cell_type":"code","source":["def seed_everything(seed:int = 1004):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","\n","seed_everything(42)\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwLrRTFkzWYu","outputId":"379aeef3-a541-460b-c219-e7052f7e6ef2","executionInfo":{"status":"ok","timestamp":1645172406844,"user_tz":-360,"elapsed":7,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"markdown","source":["### Load Tokenizer, Model\n","Hugging Face Hub에 존재하는 Pretrained Tokenizer와 Model 및 Model Config를 불러옵니다.\n","\n","이 때, Classification은 num_labels가 2로 Default되어있기 때문에 Model Config의 Parameter를 변경해줍니다."],"metadata":{"id":"cnrEI1X8Faup"}},{"cell_type":"markdown","source":["### Tokenizing\n","Train Data를 Train과 Validation Dataset으로 나누고 각각 데이터를 Tokenizer를 통해 Tokenizing을 합니다.\n","\n","Tokenizer에 들어가는 문장은 전제와 가설을 Concat한 문장이 됩니다."],"metadata":{"id":"_G-UXaLSF5g5"}},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","CV= 5\n","skf = StratifiedKFold(n_splits = CV, shuffle=True, random_state=42)\n","folds=[]\n","\n","for train_idx,val_idx in skf.split(train['index'], train['label']):\n","    folds.append((train_idx,val_idx))\n"],"metadata":{"id":"lpkSK9OlEFfy","executionInfo":{"status":"ok","timestamp":1645172455413,"user_tz":-360,"elapsed":8,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## 모델학습에 필요한 함수, Class"],"metadata":{"id":"CcBA7SbyNmM2"}},{"cell_type":"code","source":["class BERTDataset(torch.utils.data.Dataset):\n","    def __init__(self, pair_dataset, label):\n","        self.pair_dataset = pair_dataset\n","        self.label = label\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n","        item['label'] = torch.tensor(self.label[idx])\n","        \n","        return item\n","\n","    def __len__(self):\n","        return len(self.label)"],"metadata":{"id":"7K3BhRx1LasN","executionInfo":{"status":"ok","timestamp":1645172455413,"user_tz":-360,"elapsed":7,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def label_to_num(label):\n","    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n","    num_label = []\n","\n","    for v in label:\n","        num_label.append(label_dict[v])\n","    \n","    return num_label"],"metadata":{"id":"y3jP_Yl6LbTn","executionInfo":{"status":"ok","timestamp":1645172455414,"user_tz":-360,"elapsed":8,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","  \"\"\" validation을 위한 metrics function \"\"\"\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  probs = pred.predictions\n","\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n","\n","  return {\n","      'accuracy': acc,\n","  }"],"metadata":{"id":"dDqNLNLxNih2","executionInfo":{"status":"ok","timestamp":1645172455414,"user_tz":-360,"elapsed":8,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## 진짜 모델 학습"],"metadata":{"id":"Lp_lro9zNrjn"}},{"cell_type":"code","source":["# BERT config setting\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","BERT_TYPE = 'klue/roberta-large'\n","MODEL_NAME = BERT_TYPE\n","config = AutoConfig.from_pretrained(MODEL_NAME)\n","config.num_labels = 3"],"metadata":{"id":"VTfbUBRJPpiW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645182653991,"user_tz":-360,"elapsed":676,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"a6380d83-1a8a-4936-ed26-5f83def90ade"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"klue/roberta-large\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n"]}]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=os.path.join(PATH,'results'),\n","    num_train_epochs=10,\n","    per_device_train_batch_size=16,\n","    save_total_limit=2,\n","    save_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps = 100,\n","    load_best_model_at_end = True,\n",")"],"metadata":{"id":"xb8lTbM_N8dn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645182647665,"user_tz":-360,"elapsed":297,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"d2dd4062-3557-4c7a-cf49-f4cda51d78b4"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["best_models = []\n","fold = 0\n","print('===============',fold+1,'fold start===============') \n","# Get new model from fold\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)    \n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RgceJSRWmmt","executionInfo":{"status":"ok","timestamp":1645182738359,"user_tz":-360,"elapsed":6890,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"7339e90c-c388-43fb-ce9c-6b0400592fed"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["=============== 1 fold start===============\n"]},{"output_type":"stream","name":"stderr","text":["loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n","Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/360b579947002f14f22331a026821b56f70679f1be1e95fe5dc5a80edc4a59e0.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["train_idx = folds[fold][0]\n","valid_idx = folds[fold][1]\n","\n","train_data = train.loc[train_idx]\n","val_data = train.loc[valid_idx] \n","train_label = label_to_num(train_data['label'].values)\n","val_label = label_to_num(val_data['label'].values)\n","    \n","tokenized_train = tokenizer(\n","      list(train_data['premise']),\n","      list(train_data['hypothesis']),\n","      return_tensors=\"pt\",\n","      max_length=256, # Max_Length = 190\n","      padding=True,\n","      truncation=True,\n","      add_special_tokens=True\n","    )\n","tokenized_val = tokenizer(\n","      list(val_data['premise']),\n","      list(val_data['hypothesis']),\n","      return_tensors=\"pt\",\n","      max_length=256, # Max_Length = 190\n","      padding=True,\n","      truncation=True,\n","      add_special_tokens=True\n","    )\n"],"metadata":{"id":"tA9OwIPiWodB","executionInfo":{"status":"ok","timestamp":1645182763984,"user_tz":-360,"elapsed":3337,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["train_dataset = BERTDataset(tokenized_train, train_label)\n","val_dataset = BERTDataset(tokenized_val, val_label)"],"metadata":{"id":"BzEQfs1tW30U","executionInfo":{"status":"ok","timestamp":1645182774433,"user_tz":-360,"elapsed":297,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=train_dataset,\n","      eval_dataset=val_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics,\n","    )\n","\n"],"metadata":{"id":"ycPAdsSuIfmC","executionInfo":{"status":"ok","timestamp":1645182777335,"user_tz":-360,"elapsed":267,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"yjJKdCf0Xfcl","executionInfo":{"status":"error","timestamp":1645183022501,"user_tz":-360,"elapsed":1487,"user":{"displayName":"장홍선","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15052134402298649734"}},"outputId":"3de3d364-1a94-4d41-ef5e-7f0d86015077"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 22398\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 14000\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    2/14000 : < :, Epoch 0.00/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         )\n\u001b[1;32m   1215\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m                 )\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         )\n\u001b[1;32m    456\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2438\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 14.76 GiB total capacity; 12.70 GiB already allocated; 7.75 MiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["## 모델 저장"],"metadata":{"id":"9peol22XQ0yy"}}]}